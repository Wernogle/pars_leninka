from bs4 import BeautifulSoup as bs
import requests as re
import csv
import json
import time
import undetected_chromedriver

from fake_useragent import UserAgent

def through_pages():
    max_values = set()
    with open('max_pages.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for pg in row:
                max_values.add(pg)

    new_links = []
    for max_value in max_values:
        page_number = int(max_value.split('/')[-1])
        for i in range(page_number - 1, 0, -1):
            new_link = max_value.rsplit('/', 1)[0] + '/' + str(i)
            new_links.append(new_link)
            print(new_link)

    with open('links.json', 'w') as file:
        json.dump(new_links, file)

    with open('max_pages.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for pg in row:
                max_values.add(pg)

    new_links = []
    for max_value in max_values:
        page_number = int(max_value.split('/')[-1])
        for i in range(page_number - 1, 0, -1):
            new_link = max_value.rsplit('/', 1)[0] + '/' + str(i)
            new_links.append(new_link)
            print(new_link)

    with open('new_links.json', 'w') as file:
        json.dump(new_links, file)

def find_max_page():
    max_values = set()
    with open('links.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for pg in row:
                r = re.get(pg)
                soup = bs(r.text, "lxml")
                block = soup.find('a', class_='icon')
                if block is not None:
                    max_value = "https://cyberleninka.ru"+block.get('href')
                    max_values.add(max_value)
                    print(max_value)
                else:
                    print("Чёт не сработало ")

    with open('max_pages.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for value in max_values:
            writer.writerow([value])

def parse_cyberleninka():
    url_c = "https://cyberleninka.ru"

    r = re.get(url_c)
    soup = bs(r.text, "lxml")

    block_left = soup.find('div', class_='half')
    block_right = soup.find('div', class_='half-right')

    b_links = []  # Define b_links as an empty list

    for i in block_left:
        con_b_l = block_left.find_all('a')
        for link in con_b_l:
            b_links.append(url_c + link.get('href'))

    for i in block_right:
        con_b_r = block_right.find_all('a')
        for link in con_b_r:
            b_links.append(url_c + link.get('href'))

    b_links = list(set(b_links))  # Remove duplicates from b_links

    with open('links.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for i in b_links:
            writer.writerow([i])

    b_links = []  # Reset b_links as an empty list

    with open('links.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for url in row:
                r = re.get(url)
                soup = bs(r.text, "lxml")
                block = soup.find('div', class_='full')
                if block is not None:
                    con_b = block.find_all('a')
                    for link in con_b:
                        b_links.append(url_c + link.get('href'))
                        print(b_links[-1])  # Print the link in the console
                else:
                    print("Чёт не сработало ")

    b_links = list(set(b_links))  # Remove duplicates from b_links

    with open('b_links.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for link in b_links:
            writer.writerow([link])

def parse_links():
    url_c = "https://cyberleninka.ru"
    b_links = []  # Define b_links as an empty list
    ua = UserAgent()
    uagent = ua.chrome

    with open('new_links.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for url in row:
                headers = {'User-Agent': uagent}
                r = re.get(url, headers=headers)
                soup = bs(r.text, "lxml")
                block = soup.find('div', class_='full')
                if block is not None:
                    con_b = block.find_all('a')
                    for link in con_b:
                        b_links.append(url_c + link.get('href'))
                        print(b_links[-1])  # Print the link in the console
                else:
                    print("\033[1;31mЧёт не сработало\033[0m")
                    try:
                        driver = undetected_chromedriver.Chrome()
                        driver.get(url)
                        time.sleep(50)
                    except Exception as ex:
                        print(ex)
                    finally:
                        driver.close()
                        driver.quit()

    b_links = list(set(b_links))  # Remove duplicates from b_links

    with open('b_links.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for link in b_links:
            writer.writerow([link])

parse_links()
