from bs4 import BeautifulSoup as bs
import requests as re
import csv
import json
import pandas as pd
import time
import undetected_chromedriver


import json

def through_pages():
    max_values = set()
    with open('max_pages.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for pg in row:
                max_values.add(pg)

    new_links = []
    for max_value in max_values:
        page_number = int(max_value.split('/')[-1])
        for i in range(page_number - 1, 0, -1):
            new_link = max_value.rsplit('/', 1)[0] + '/' + str(i)
            new_links.append(new_link)
            print(new_link)

    with open('links.json', 'w') as file:
        json.dump(new_links, file)

    with open('max_pages.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for pg in row:
                max_values.add(pg)

    new_links = []
    for max_value in max_values:
        page_number = int(max_value.split('/')[-1])
        for i in range(page_number - 1, 0, -1):
            new_link = max_value.rsplit('/', 1)[0] + '/' + str(i)
            new_links.append(new_link)
            print(new_link)

    with open('new_links.json', 'w') as file:
        json.dump(new_links, file)

def find_max_page():
    max_values = set()
    with open('links.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for pg in row:
                r = re.get(pg)
                soup = bs(r.text, "lxml")
                block = soup.find('a', class_='icon')
                if block is not None:
                    max_value = "https://cyberleninka.ru"+block.get('href')
                    max_values.add(max_value)
                    print(max_value)
                else:
                    print("Чёт не сработало ")
                    # try:
                    #     driver = undetected_chromedriver.Chrome()
                    #     driver.get(pg)
                    #     time.sleep(50)
                    # except Exception as ex:
                    #     print(ex)
                    # finally:
                    #     driver.close()
                    #     driver.quit()

    with open('max_pages.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for value in max_values:
            writer.writerow([value])


def parse_cyberleninka():
    url_c = "https://cyberleninka.ru"

    # Получаем ссылки с главной страницы
    r = re.get(url_c)
    soup = bs(r.text, "lxml")

    block_left = soup.find('div', class_='half')
    block_right = soup.find('div', class_='half-right')

    b_l = []
    b_r = []

    for i in block_left:
        con_b_l = block_left.find_all('a')
        for link in con_b_l:
            b_l.append(url_c + link.get('href'))

    for i in block_right:
        con_b_r = block_right.find_all('a')
        for link in con_b_r:
            b_r.append(url_c + link.get('href'))

    b_l = list(set(b_l))  # удаляем дубликаты из списка
    b_r = list(set(b_r))  # удаляем дубликаты из списка

    # Записываем ссылки в файл links.csv
    with open('links.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for i in b_l:
            writer.writerow([i])
        for j in b_r:
            writer.writerow([j])

    # Парсим страницы и получаем новые ссылки
    b_links = []
    with open('links.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for url in row:
                r = re.get(url)
                soup = bs(r.text, "lxml")
                block = soup.find('div', class_='full')
                if block is not None:
                    con_b = block.find_all('a')
                    for link in con_b:
                        # добавляем ссылку в список
                        b_links.append(url_c + link.get('href'))
                        print(b_links[-1])  # выводим ссылку в консоль
                else:
                    print("Чёт не сработало ")
                    continue
                    # try:
                    #     driver = undetected_chromedriver.Chrome()
                    #     driver.get(url)
                    #     time.sleep(50)
                    # except Exception as ex:
                    #     print(ex)
                    # finally:
                    #     driver.close()
                    #     driver.quit()

    b_links = list(set(b_links))  # удаляем дубликаты из списка

    # Записываем новые ссылки в файл b_links.csv
    with open('b_links.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for link in b_links:
            writer.writerow([link])

def parse_links():
    url_c = "https://cyberleninka.ru"
    with open('new_links.csv', 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            for url in row:
                r = re.get(url)
                soup = bs(r.text, "lxml")
                block = soup.find('div', class_='full')
                if block is not None:
                    con_b = block.find_all('a')
                    for link in con_b:
                        print(url_c + link.get('href'))
                else:
                    print("\033[1;31mЧёт не сработало\033[0m")
                    try:
                        driver = undetected_chromedriver.Chrome()
                        driver.get(url)
                        time.sleep(50)
                    except Exception as ex:
                        print(ex)
                    finally:
                        driver.close()
                        driver.quit()
                    continue
                time.sleep(50)

    with open('b_links.csv', 'w', newline='') as file:
        writer = csv.writer(file)
        for link in b_links:
            writer.writerow([link])

parse_links()
